{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38a5279-4ebc-4be1-833e-80171014cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306db759-936b-45b9-9d0e-d055dc08cc79",
   "metadata": {},
   "source": [
    "https://kimi.moonshot.cn/chat/co3af54udu62gncaf9fg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d522547-2bc7-4858-8727-8b24f0fb40d1",
   "metadata": {},
   "source": [
    "分布式数据并行（Distributed Data Parallel，简称DDP）是PyTorch中用于分布式训练的一种方法，它允许在多个进程或多个机器上并行地训练模型。每个进程拥有模型的一个副本，并且每个进程使用不同的数据子集进行训练。在训练过程中，各个进程会异步更新模型参数，并通过某种通信机制（如NVIDIA的NCCL库）同步梯度或参数更新。\n",
    "\n",
    "以下是一个使用PyTorch的torch.nn.parallel.DistributedDataParallel（DDP）的简单测试例子："
   ]
  },
  {
   "cell_type": "raw",
   "id": "03dc3952-bf19-47b2-bf71-bd7cca3e33de",
   "metadata": {},
   "source": [
    "--nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=172.17.0.2 --master_port=50574\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243a6e4-8dd4-4c79-8b8f-a627d8d2207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.rand(10), torch.rand(10)\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = '172.17.0.2'\n",
    "    os.environ['MASTER_PORT'] = '50574'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    dataset = ToyDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=10)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        for i, (inputs, labels) in enumerat(dataloader):\n",
    "            inputs = inputs.to(rank)\n",
    "            labels = labels.to(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(f'rank: {}, epoch: {epoch}, iteration:{i}, loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    rank = int(sys.argv[1])\n",
    "    world_size = int(sys.argv[2])\n",
    "    train(rank, world_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607795d0-6f0f-4f78-8d32-5de3e724ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: torchrun: command not found\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"172.17.0.2\" --master_port=50574 tst_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb5cd1-6a32-499d-aa35-8415df4de8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_py3",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
